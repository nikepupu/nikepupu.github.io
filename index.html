<!DOCTYPE html>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
	<!-- Global site tag (gtag.js) - Google Analytics -->
	<script async src="https://www.googletagmanager.com/gtag/js?id=G-VPZ41Q777R"></script>
	<script>
	  window.dataLayer = window.dataLayer || [];
	  function gtag(){dataLayer.push(arguments);}
	  gtag('js', new Date());

	  gtag('config', 'G-VPZ41Q777R');
	</script>
    
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="description" content="Ran&#39;s Blog">
    <meta name="author" content="Ran Gong">

    <title>Ran Gong</title>

    <!-- Bootstrap Core CSS -->
    <link href="./css/bootstrap.min.css" rel="stylesheet">

    <!-- Main CSS -->
    <link href="./css/main.css" rel="stylesheet">

    <!-- Font Awesome CSS -->
    <link rel="stylesheet" href="./css/font-awesome.min.css">

    <!-- icon -->
    <link rel="shortcut icon" href="imgs/rangong.jpeg" type="image/x-icon">
    <link rel="icon" href="imgs/rangong.jpeg" type="image/x-icon">

    <!-- Google Fonts -->
    <link rel="stylesheet" href="./css/css">

    <!-- HTML5 Shim and Respond.js IE8 support of HTML5 elements and media queries -->
    <!-- WARNING: Respond.js doesn't work if you view the page via file:// -->
    <!--[if lt IE 9]>
        <script src="https://oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script>
        <script src="https://oss.maxcdn.com/libs/respond.js/1.4.2/respond.min.js"></script>
    <![endif]-->

</head>

<!-- The #page-top ID is part of the scrolling feature - the data-spy and data-target are part of the built-in Bootstrap scrollspy function -->

<body id="page-top" data-spy="scroll" data-target=".navbar-fixed-top">

    <!-- Navigation -->
    <nav class="navbar navbar-default navbar-fixed-top navbar-inverse bg-inverse top-nav-collapse" role="navigation">
        <div class="container">
            <div class="navbar-header page-scroll">
                <button type="button" class="navbar-toggle" data-toggle="collapse" data-target=".navbar-ex1-collapse">
                    <span class="sr-only">Toggle navigation</span>
                    <span class="icon-bar"></span>
                    <span class="icon-bar"></span>
                    <span class="icon-bar"></span>
                </button>
                <a class="navbar-brand page-scroll" href="http://nikepupu.github.io/#page-top">Ran Gong</a>
            </div>

            <!-- Collect the nav links, forms, and other content for toggling -->
            <div class="collapse navbar-collapse navbar-ex1-collapse">
                <ul class="nav navbar-nav navbar-right">
                    <!-- Hidden li included to remove active class from about link when scrolled up past about section -->
                    <li class="hidden active">
                        <a class="page-scroll" href="http://nikepupu.github.io/#page-top"></a>
                    </li>
                    <li class="">
                        <a class="page-scroll" href="http://nikepupu.github.io/#about">ABOUT</a>
                    </li>
<!--                     <li>
                        <a class="page-scroll" href="#projects">Projects</a>
                    </li> -->
                    <li class="">
                        <a class="page-scroll" href="http://nikepupu.github.io/#publications">PUBLICATIONS</a>
                    </li>
                    <!-- <li>
                        <a class="page-scroll" href="#awards">Awards</a>
                    </li> -->
                    <li>
                        <a class="page-scroll" href="https://nikepupu.github.io/CV_RanGong_Oct2023.pdf">CV</a>
                    </li>
                </ul>
<!--                 <ul class="nav navbar-nav navbar-right">
                    <li>
                        <a href="mailto:xfgao@ucla.edu" class="navbar-brand"><i class="fa fa-envelope"></i></a>
                    </li>
                    <li>
                        <a href="https://github.com/xfgao/" class="navbar-brand"><i class="fa fa-github"></i></a>
                    </li>
                    <li>
                        <a href="https://scholar.google.com/citations?user=AjTfCjEAAAAJ&hl=en&authuser=1" class="navbar-brand"><i class="fa fa-graduation-cap"></i></a>
                    </li>
                    <li>
                        <a href="https://www.linkedin.com/in/xiaofeng-gao-09b966115/" class="navbar-brand"><i class="fa fa-linkedin"></i></a>
                    </li>
                    <li>
                        <a href="https://www.facebook.com/profile.php?id=100010717656664" class="navbar-brand"><i class="fa fa-facebook"></i></a>
                    </li>

                </ul> -->

            </div>
            <!-- /.navbar-collapse -->
        </div>
        <!-- /.container -->
    </nav>

    <!-- Intro Section -->
    <section id="intro" class="intro-section">
        <div class="container">
            <div class="row">
                <div class="col-xs-12 col-sm-6 col-md-5 col-lg-4">
                        <img src="./imgs/rangong.jpeg" class="img-responsive img-circle img-fluid" width="250" align="right">
                </div>
                <div class="col-xs-12 col-md-6 col-md-7 col-lg-8 personal-intro">
                    <div class="col-xs-12 col-sm-12 col-md-12">
                        <h2 class="text-uppercase">Ran Gong</h2>
                        <address>
                            1855 Westwood Blvd<br>
                            Los Angeles, CA, 90025
                        </address>
                        <span><strong>Email: </strong>nikepupu at ucla dot edu</span>
                        <br>
                        <a href="https://scholar.google.com/citations?user=ApkezJ0AAAAJ&hl=en&authuser=1">[Google Scholar]</a>&nbsp;&nbsp;
						<a href="https://github.com/nikepupu">[GitHub]</a>
                    </div>
					
                </div>
            </div>
        </div>
    </section>
    <hr>
    <!-- About Section -->
    <section id="about" class="about-section">
        <div class="container">
            <h1>About</h1>
            <div class="bio-info">
                <p> I'm a Phd Student at the University of California Los Angeles. My research lies in the intersection of Robotics, Computer Vision, Machine Learning. I am supervised by professor Song-chun Zhu. 

                <p> During my PhD, I also worked closely with <a href="https://web.cs.ucla.edu/~dt/">Prof. Demetri Terzopoulos</a> (UCLA), <a href="https://viterbi.usc.edu/directory/faculty/Sukhatme/Gaurav">Prof. Gaurav Sukhatme</a> (USC & Amazon Alexa AI) and <a href="http://tshu.io">Dr. Tianmin Shu</a> (MIT). Before that, I obtained a bachelor degree of Computer Science and Engineering at <a href="https://my.ucla.edu/"> UCLA</a></p>
            </div>

        </div>
    </section>
    <hr>

    <!-- News Section -->
    <section id="news" class="news-section">
        <div class="container">
        	<h1>News</h1>
            <div class="news">
                <p><strong>08/2023</strong>: LEMMA is accepted by RA-L.
                <p><strong>07/2023</strong>: ARNOLD is accepted by ICCV 2023.
            	<p><strong>12/2022</strong>: ARNOLD is accepted as Spotlight by <a href="https://sites.google.com/view/langrob-corl22/">CORL 2022 Workshop on Language and Robot Learning</a>.
            </div>
        </div>
    </section>
    <hr></hr>


    <section id="publications" class="publications-section">
        <div class="container">
            <div class="row">
                <div class="col-lg-12">
                    <h1>Publications</h1>
                        <div class="contents">
                            <ul class="list-group">
                            	(* indicates equal contribution)

                                <!-- LEMMA -->
                                <li class="list-group-item">
                                    <div class="row">
                                        <div class="col-xs-12 col-sm-4 col-lg-2">
                                            <img src="./imgs/lemma.png" class="img-responsive img-fluid">
                                        </div>
                                        <div class="col-xs-12 col-sm-8 col-lg-10">
                                        <h4 class="list-group-item-heading">LEMMA: Learning Language-Conditioned Multi-Robot Manipulation</h4>
                                        <p class="detail">
                                            <strong>Ran Gong</strong>,
                                            Xiaofeng Gao, 
                                            Qiaozi Gao,                                       
                                            Suhaila Shakiah,
                                            Govind Thattai,
                                            Gaurav S. Sukhatme <br>
                                            <em> IEEE Robotics and Automation Letters (RA-L), 2023</em>
                                        </p>
                                        <button class="btn btn-success btn-xs" data-toggle="collapse" data-target="#lemma-abs">Abstract</button>
                                        <a class="btn btn-primary btn-xs" role="button" href="https://xfgao.github.io/paper/RAL2023_LEMMA.pdf">PDF</a>
                                        <button class="btn btn-warning btn-xs" data-toggle="collapse" data-target="#lemma-cite">Cite</button>
                                        <a class="btn btn-warning btn-xs" role="button" href="https://lemma-benchmark.github.io/">Website</a>
                                        </div>
                                    </div>
                                    <div id="lemma-abs" class="collapse abstract">
                                        Complex manipulation tasks often require robots with complementary capabilities to collaborate. We introduce a benchmark for LanguagE-Conditioned Multi-robot MAnipulation (LEMMA) focused on task allocation and long-horizon object manipulation based on human language instructions in a tabletop setting. LEMMA features 8 types of procedurally generated tasks with varying degree of complexity, some of which require the robots to use tools and pass tools to each other. For each task, we provide 800 expert demonstrations and human instructions for training and evaluations. LEMMA poses greater challenges compared to existing benchmarks, as it requires the system to identify each manipulator's limitations and assign sub-tasks accordingly while also handling strong temporal dependencies in each task. To address these challenges, we propose a modular hierarchical planning approach as a baseline. Our results highlight the potential of LEMMA for developing future language-conditioned multi-robot systems.
                                    </div>

                                    <div id="lemma-cite" class="collapse abstract">
                                        <pre class="citation" align="left">
@article{gong2023lemma,
  title={LEMMA: Learning Language-Conditioned Multi-Robot Manipulation},
  author={Gong, Ran and Gao, Xiaofeng and Gao, Qiaozi and Shakiah, Suhaila and Thattai, Govind and Sukhatme, Gaurav S},
  journal={arXiv preprint arXiv:2308.00937},
  year={2023}
}</pre>
                                    </div>  
                                </li>

        
                                <!-- Arnold -->
                                <li class="list-group-item">
                                    <div class="row">
                                        <div class="col-xs-12 col-sm-4 col-lg-2">
                                            <img src="./imgs/arnold.gif" class="img-responsive img-fluid">
                                        </div>
                                        <div class="col-xs-12 col-sm-8 col-lg-10">
                                        <h4 class="list-group-item-heading">ARNOLD: A Benchmark for Language-Grounded Task Learning with Continuous States in Realistic Scenes</h4>
                                        <p class="detail">
                                            <strong>Ran Gong</strong>*,
                                            <a href="https://huangjy-pku.github.io/">Jiangyong Huang*</a>,
                                            <a href="https://www.zyz.lol/app/aboutme/aboutme.html">Yizhou Zhao</a>,
                                            <a href="https://geng-haoran.github.io/">Haoran Geng</a>,
                                            <a href="https://xfgao.github.io/"> Xiaofeng Gao </a>, 
                                            <a href="https://qywu.github.io/">Qingyang Wu</a>,
                                            <a href="https://wensi-ai.github.io/">Wensi Ai</a>,
                                            <a href="https://www.linkedin.com/in/josephziheng/">Ziheng Zhou</a>,
                                            <a href="http://web.cs.ucla.edu/~dt/">Demetri Terzopoulos</a>,
                                            <a href="http://www.stat.ucla.edu/~sczhu/">Song-Chun Zhu</a>,
                                            <a href="https://buzz-beater.github.io/">Baoxiong Jia</a>,
                                            <a href="https://siyuanhuang.com/">Siyuan Huang</a> <br>
                                            <em> International Conference on Computer Vision (ICCV), 2023</em>
                                        </p>
                                        <button class="btn btn-success btn-xs" data-toggle="collapse" data-target="#arnold-abs">Abstract</button>
                                        <a class="btn btn-primary btn-xs" role="button" href="https://arxiv.org/abs/2304.04321">Paper</a>
                                        <button class="btn btn-warning btn-xs" data-toggle="collapse" data-target="#arnold-cite">Cite</button>
                                        <a class="btn btn-warning btn-xs" role="button" href="https://arnold-benchmark.github.io/">Website</a>
                                        </div>
                                    </div>
                                    <div id="arnold-abs" class="collapse abstract">
                                        Understanding the continuous states of objects is essential for task learning and planning in the real world. However, most existing task learning benchmarks assume discrete(e.g., binary) object goal states, which poses challenges for the learning of complex tasks and transferring learned policy from simulated environments to the real world. Furthermore, state discretization limits a robot's ability to follow human instructions based on the grounding of actions and states. To tackle these challenges, we present ARNOLD, a benchmark that evaluates language-grounded task learning with continuous states in realistic 3D scenes. ARNOLD is comprised of 8 language-conditioned tasks that involve understanding object states and learning policies for continuous goals. To promote language-instructed learning, we provide expert demonstrations with template-generated language descriptions. We assess task performance by utilizing the latest language-conditioned policy learning models. Our results indicate that current models for language-conditioned manipulations continue to experience significant challenges in novel goal-state generalizations, scene generalizations, and object generalizations. These findings highlight the need to develop new algorithms that address this gap and underscore the potential for further research in this area.
                                    </div>

                                    <div id="arnold-cite" class="collapse abstract">
                                        <pre class="citation" align="left">
@article{gong2023arnold,
  title={ARNOLD: A Benchmark for Language-Grounded Task Learning With Continuous States in Realistic 3D Scenes},
  author={Gong, Ran and Huang, Jiangyong and Zhao, Yizhou and Geng, Haoran and Gao, Xiaofeng and Wu, Qingyang and Ai, Wensi and Zhou, Ziheng and Terzopoulos, Demetri and Zhu, Song-Chun and others},
  journal={arXiv preprint arXiv:2304.04321},
  year={2023}
}</pre>
                                    </div>  
                                </li>


                                <!-- DialFRED -->
                                <li class="list-group-item">
                                    <div class="row">
                                        <div class="col-xs-12 col-sm-4 col-lg-2">
                                            <img src="./imgs/dialfred_intro.png" class="img-responsive img-fluid">
                                        </div>
                                        <div class="col-xs-12 col-sm-8 col-lg-10">
                                        <h4 class="list-group-item-heading">DialFRED: Dialogue-Enabled Agents for Embodied Instruction Following</h4>
                                        <p class="detail">
                                            Xiaofeng Gao, 
                                            Qiaozi Gao,
											<strong>Ran Gong</strong>,
                                            Kaixiang Lin,
                                            Govind Thattai,
                                            Gaurav S. Sukhatme <br>
                                            <em> IEEE Robotics and Automation Letters (RA-L), 2022</em>
                                        </p>
                                        <button class="btn btn-success btn-xs" data-toggle="collapse" data-target="#dialfred-abs">Abstract</button>
                                        <a class="btn btn-primary btn-xs" role="button" href="https://xfgao.github.io/paper/RAL2022_DialFRED.pdf">PDF</a>
                                        <button class="btn btn-warning btn-xs" data-toggle="collapse" data-target="#dialfred-cite">Cite</button>
                                        <a class="btn btn-warning btn-xs" role="button" href="https://github.com/xfgao/DialFRED">Code&Data</a>
                                        </div>
                                    </div>
                                    <div id="dialfred-abs" class="collapse abstract">
                                        Language-guided Embodied AI benchmarks requiring an agent to navigate an environment and manipulate objects typically allow one-way communication: the human user gives a natural language command to the agent, and the agent can only follow the command passively. We present DialFRED, a dialogue-enabled embodied instruction following benchmark based on the ALFRED benchmark. DialFRED allows an agent to actively ask questions to the human user; the additional information in the user's response is used by the agent to better complete its task. We release a human-annotated dataset with 53K task-relevant questions and answers and an oracle to answer questions. To solve DialFRED, we propose a questioner-performer framework wherein the questioner is pre-trained with the human-annotated data and fine-tuned with reinforcement learning. We make DialFRED publicly available and encourage researchers to propose and evaluate their solutions to building dialog-enabled embodied agents.
                                    </div>
                                    <div id="dialfred-cite" class="collapse abstract">
                                        <pre class="citation" align="left">
@article{gao2022dialfred,
  title={DialFRED: Dialogue-Enabled Agents for Embodied Instruction Following}, 
  author={Gao, Xiaofeng and Gao, Qiaozi and Gong, Ran and Lin, Kaixiang and Thattai, Govind and Sukhatme, Gaurav S.},
  journal={IEEE Robotics and Automation Letters}, 
  year={2022},
  volume={7},	
  number={4},
  pages={10049-10056},
  doi={10.1109/LRA.2022.3193254}
}</pre>
                                    </div>  
                                </li>
 <!-- Inter-GPS -->
 <li class="list-group-item">
	<div class="row">
		<div class="col-xs-12 col-sm-4 col-lg-2">
			<img src="./imgs/intergps_teaser.png" class="img-responsive img-fluid">
		</div>
		<div class="col-xs-12 col-sm-8 col-lg-10">
		<h4 class="list-group-item-heading">Inter-GPS: Interpretable Geometry Problem Solving with Formal Language and Symbolic Reasoning</h4>
		<p class="detail">
			Pan Lu*, 
			<strong>Ran Gong*</strong>,
			Shibiao Jiang,
			Liang Qiu,
			Siyuan Huang,
			Xiaodan Liang,
			Song-Chun Zhu<br>
			<em> The Association for Computational Linguistics 2021 Oral Presentation </em>
		</p>
		<button class="btn btn-success btn-xs" data-toggle="collapse" data-target="#intergps-abs">Abstract</button>
		<a class="btn btn-primary btn-xs" role="button" href="https://nikepupu.github.io/paper/acl21_intergps.pdf">PDF</a>
		<button class="btn btn-warning btn-xs" data-toggle="collapse" data-target="#intergps-cite">Cite</button>
		<a class="btn btn-warning btn-xs" role="button" href="https://github.com/lupantech/InterGPS">Code&Data</a>
		</div>
	</div>
	<div id="intergps-abs" class="collapse abstract">
		Geometry problem solving has attracted much attention in the NLP community recently. The task is challenging as it requires abstract problem understanding and symbolic reasoning with axiomatic knowledge. However, current datasets are either small in scale or not publicly available. Thus, we construct a new largescale benchmark, Geometry3K, consisting of 3,002 geometry problems with dense annotation in formal language. We further propose a novel geometry solving approach with formal language and symbolic reasoning, called Interpretable Geometry Problem Solver (InterGPS). Inter-GPS first parses the problem text and diagram into formal language automatically via rule-based text parsing and neural object detecting, respectively. Unlike implicit 	learning in existing methods, Inter-GPS incorporates theorem knowledge as conditional rules and performs symbolic reasoning step by step. Also, a theorem predictor is designed to infer the theorem application sequence fed to the symbolic solver for the more efficient and reasonable searching path. Extensive experiments on the Geometry3K and GEOS datasets demonstrate that Inter-GPS achieves significant improvements over existing methods
	</div>
	<div id="intergps-cite" class="collapse abstract">
		<pre class="citation" align="left">
			@inproceedings{lu2021inter,
				title={Inter-GPS: Interpretable Geometry Problem Solving with Formal Language and Symbolic Reasoning},
				author={Lu, Pan and Gong, Ran and Jiang, Shibiao and Qiu, Liang and Huang, Siyuan and Liang, Xiaodan and Zhu, Song-Chun},
				booktitle={The 59th Annual Meeting of the Association for Computational Linguistics (ACL)},
				year={2021}
			  }</pre>
	</div>  
</li>


 <!-- Smart -->
 <li class="list-group-item">
	<div class="row">
		<div class="col-xs-12 col-sm-4 col-lg-2">
			<img src="./imgs/smart.png" class="img-responsive img-fluid">
		</div>
		<div class="col-xs-12 col-sm-8 col-lg-10">
		<h4 class="list-group-item-heading">SMART: A Situation Model for Algebra Story Problems via Attributed Grammar</h4>
		<p class="detail">
			Yining Hong, 
			Qing Li,
			<strong>Ran Gong</strong>,
			Daniel Ciao,
			Siyuan Huang,
			Song-Chun Zhu<br>
			<em> Association for the Advancement of Artificial Intelligence 2021 </em>
		</p>
		<button class="btn btn-success btn-xs" data-toggle="collapse" data-target="#smart-abs">Abstract</button>
		<a class="btn btn-primary btn-xs" role="button" href="https://nikepupu.github.io/paper/smart.pdf">PDF</a>
		<button class="btn btn-warning btn-xs" data-toggle="collapse" data-target="#smart-cite">Cite</button>
		<a class="btn btn-warning btn-xs" role="button" href="https://evelinehong.github.io/smart-site/">Code&Data</a>
		</div>
	</div>
	<div id="smart-abs" class="collapse abstract">
		Solving algebra story problems remains a challenging task in artificial intelligence, which requires a detailed understanding of real-world situations and a strong mathematical reasoning capability. Previous neural solvers of math word problems directly translate problem texts into equations, lacking an explicit interpretation of the situations, and often fail to handle more sophisticated situations. To address such limits of neural solvers, we introduce the concept of a situation model, which originates from psychology studies to represent the mental states of humans in problem-solving, and propose SMART, which adopts attributed grammar as the representation of situation models for algebra story problems. Specifically, we first train an information extraction module to extract nodes, attributes, and relations from problem texts and then generate a parse graph based on a pre-defined attributed grammar. An iterative learning strategy is also proposed to improve the performance of SMART further. To rigorously study this task, we carefully curate a new dataset named ASP6.6k. Experimental results on ASP6.6k show that the proposed model outperforms all previous neural solvers by a large margin while preserving much better interpretability. To test these models' generalization capability, we also design an out-of-distribution (OOD) evaluation, in which problems are more complex than those in the training set. Our model exceeds state-of-the-art models by 17% in the OOD evaluation, demonstrating its superior generalization ability.
	</div>
	<div id="smart-cite" class="collapse abstract">
		<pre class="citation" align="left">
			@inproceedings{hong2021smart,
				title={SMART: A Situation Model for Algebra Story Problems via Attributed Grammar},
				author={Hong, Yining and Li, Qing and Gong, Ran and Ciao, Daniel and Huang, Siyuan and Zhu, Song-Chun.},
				booktitle={The Thirty-Fifth AAAI Conference on Artificial Intelligence, {AAAI-21}},
				year={2021}
			}</pre>
	</div>  
</li>

 <!-- Attention -->
 <li class="list-group-item">
	<div class="row">
		<div class="col-xs-12 col-sm-4 col-lg-2">
			<img src="./imgs/attention.png" class="img-responsive img-fluid">
		</div>
		<div class="col-xs-12 col-sm-8 col-lg-10">
		<h4 class="list-group-item-heading"> Learning to Infer Human Attention in Daily Activities</h4>
		<p class="detail">
			Zhixiong Nan, 
			Tianmin Shu,
			<strong>Ran Gong</strong>,
			Shu Wang,
			Ping Wei,
			Song-Chun Zhu,
			Nanning Zheng<br>
			<em> Pattern Recognition 2020 </em>
		</p>
		<button class="btn btn-success btn-xs" data-toggle="collapse" data-target="#attention-abs">Abstract</button>
		<a class="btn btn-primary btn-xs" role="button" href="https://nikepupu.github.io/paper/PR_Infer.pdf">PDF</a>
		<button class="btn btn-warning btn-xs" data-toggle="collapse" data-target="#attention-cite">Cite</button>
		</div>
	</div>
	<div id="attention-abs" class="collapse abstract">
		The first attention model in the computer science community is proposed in 1998. In the following years, human attention has been intensively studied. However, these studies mainly refer human attention as the image regions that draw the attention of a human (outside the image) who is looking at the image. In this paper, we infer the attention of a human inside a third-person view video where the human is doing a task, and define human attention as attentional objects that coincide with the task the human is doing. To infer human attention, we propose a deep neural network model that fuses both low-level human pose cue and high-level task encoding cue. Due to the lack of appropriate public datasets for studying this problem, we newly collect a video dataset in complex Virtual-Reality (VR) scenes. In the experiments, we widely compare our method with three other methods on this VR dataset. In addition, we re-annotate a public real dataset and conduct the extensional experiments on this real dataset. The experiment results validate the effectiveness of our method
	</div>
	<div id="attention-cite" class="collapse abstract">
		<pre class="citation" align="left">
			@article{nan2020learning,
				title={Learning to infer human attention in daily activities},
				author={Nan, Zhixiong and Shu, Tianmin and Gong, Ran and Wang, Shu and Wei, Ping and Zhu, Song-Chun and Zheng, Nanning},
				journal={Pattern Recognition},
				volume={103},
				pages={107314},
				year={2020},
				publisher={Elsevier}
			  }
			}</pre>
	</div>  
</li>


                               
                                <!-- XCooking -->
                                <li class="list-group-item">
                                    <div class="row">
                                        <div class="col-xs-12 col-sm-4 col-lg-2">
                                            <img src="./imgs/map_final.png" class="img-responsive img-fluid">
                                        </div>
                                        <div class="col-xs-12 col-sm-8 col-lg-10">
                                        <h4 class="list-group-item-heading">Joint Mind Modeling for Explanation Generation in Complex Human-Robot Collaborative Tasks</h4>
                                        <p class="detail">
                                            <a href="https://xfgao.github.io/">Xiaofeng Gao* </a>, 
                                            <strong>Ran Gong*</strong>,
                                            <a href="https://yizhouzhao.github.io/">Yizhou Zhao</a>,
                                            <a href="https://shuwang0712.github.io/">Shu Wang</a>,
                                            <a href="https://tshu.io">Tianmin Shu</a>,
                                            <a href="http://www.stat.ucla.edu/~sczhu/">Song-Chun Zhu</a><br>
                                            <em> IEEE International Conference on Robot & Human Interactive Communication (RO-MAN), 2020 </em>
                                        </p>
                                        <button class="btn btn-success btn-xs" data-toggle="collapse" data-target="#XCooking-abs">Abstract</button>
                                        <a class="btn btn-primary btn-xs" role="button" href="https://xfgao.github.io/paper/ROMAN2020_XCooking.pdf">PDF</a>
                                        <button class="btn btn-warning btn-xs" data-toggle="collapse" data-target="#XCooking-cite">Cite</button>
                                        <a class="btn btn-warning btn-xs" role="button" href="https://xfgao.github.io/xCookingWeb/">Website</a> 
                                        <a class="btn btn-warning btn-xs" role="button" href="https://youtu.be/Q8jmEQ6JqQ0">Talk</a>
                                        <a class="btn btn-warning btn-xs" role="button" href="https://xfgao.github.io/slides/ROMAN20_collabCooking.pdf">Slides</a>
                                        </div>
                                    </div>
                                    <div id="XCooking-abs" class="collapse abstract">
                                        Human collaborators can effectively communicate with their partners to finish a common task by inferring each other's mental states (e.g., goals, beliefs, and desires). Such mind-aware communication minimizes the discrepancy among collaborators' mental states, and is crucial to the success in human ad-hoc teaming. We believe that robots collaborating with human users should demonstrate similar pedagogic behavior. Thus, in this paper, we propose a novel explainable AI (XAI) framework for achieving human-like communication in human-robot collaborations, where the robot builds a hierarchical mind model of the human user and generates explanations of its own mind as a form of communications based on its online Bayesian inference of the user's mental state. To evaluate our framework, we conduct a user study on a real-time human-robot cooking task. Experimental results show that the generated explanations of our approach significantly improves the collaboration performance and user perception of the robot.
                                    </div>
                                    <div id="XCooking-cite" class="collapse abstract">
                                        <pre class="citation" align="left">
@inproceedings{gao2020joint,
  title={Joint Mind Modeling for Explanation Generation in Complex Human-Robot Collaborative Tasks},
  author={Gao, Xiaofeng and Gong, Ran and Zhao, Yizhou and Wang, Shu and Shu, Tianmin and Zhu, Song-Chun},
  booktitle={2020 29th IEEE International Conference on Robot and Human Interactive Communication (RO-MAN)},
  pages={1119--1126},
  year={2020},
  organization={IEEE}
}</pre>

                                    </div>  
                                </li>

                                <!-- VRKitchen -->
                                <li class="list-group-item">
                                    <div class="row">
                                        <div class="col-xs-12 col-sm-4 col-lg-2">
                                            <img src="./imgs/dish_fluent_small.gif" class="img-responsive img-fluid">
                                        </div>
                                        <div class="col-xs-12 col-sm-8 col-lg-10">
                                        <h4 class="list-group-item-heading">VRKitchen: an Interactive 3D Environment for Learning Real Life Cooking Tasks</h4>
                                        <p class="detail">
                                            <a href="https://xfgao.github.io/"> Xiaofeng Gao </a>, 
                                            <strong>Ran Gong </strong>,
                                            <a href="https://tshu.io">Tianmin Shu</a>,
                                            <a href="https://xuxie1031.github.io/">Xu Xie</a>,
                                            <a href="https://shuwang0712.github.io/">Shu Wang</a>,
                                            <a href="http://www.stat.ucla.edu/~sczhu/">Song-Chun Zhu</a><br>
                                            <em> ICML workshop on Reinforcement Learning for Real Life, 2019 </em>
                                        </p>
                                        <button class="btn btn-success btn-xs" data-toggle="collapse" data-target="#kitchen-abs">Abstract</button>
                                        <a class="btn btn-primary btn-xs" role="button" href="https://xfgao.github.io/paper/Arxiv2019_VRKitchen.pdf">PDF</a>
                                        <button class="btn btn-warning btn-xs" data-toggle="collapse" data-target="#kitchen-cite">Cite</button>
                                        <a class="btn btn-warning btn-xs" role="button" href="https://sites.google.com/view/vr-kitchen/">Website</a> 
                                        </div>
                                    </div>
                                    <div id="kitchen-abs" class="collapse abstract">
                                        One of the main challenges of applying reinforcement learning to real world applications is the lack of realistic and standardized environments for training and testing AI agents. In this work, we design and implement a virtual reality (VR) system, VRKitchen, with integrated functions which i) enable embodied agents to perform real life cooking tasks involving a wide range of object manipulations and state changes, and ii) allow human teachers to provide demonstrations for training agents. We also provide standardized evaluation benchmarks and data collection tools to facilitate a broad use in research on learning real life tasks. Video demos, code, and data will be available on the project website: sites.google.com/view/vr-kitchen.
                                    </div>
                                    <div id="kitchen-cite" class="collapse abstract">
                                        <pre class="citation" align="left">
@article{gao2019vrkitchen,
  title={Vrkitchen: an interactive 3d virtual environment for task-oriented learning},
  author={Gao, Xiaofeng and Gong, Ran and Shu, Tianmin and Xie, Xu and Wang, Shu and Zhu, Song-Chun},
  journal={arXiv preprint arXiv:1903.05757},
  year={2019}
}</pre>

                                    </div>  
                                </li>


                                    </div>	
                                </li>


                            </ul>
                        </div>
                </div>
            </div>
        </div>
    </section>
    <hr>

    <footer class="footer">
        <div class="container">
            <p class="text-muted">© 2023 All rights reserved. Developed by Ran Gong.</p>
        </div>
    </footer>

    <!-- jQuery -->
    <script src="./css/jquery.js"></script>

    <!-- Bootstrap Core JavaScript -->
    <script src="./css/bootstrap.min.js"></script>

    <!-- Scrolling Nav JavaScript -->
    <script src="./css/jquery.easing.min.js"></script>
    <script src="./css/scrolling-nav.js"></script>




</body></html>
